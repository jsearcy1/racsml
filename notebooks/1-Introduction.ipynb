{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "* Machine Learning refers to a number of different methods, but all have the goal of approximating a function.\n",
    "\n",
    "### Goal \n",
    "* In this lesson we will build and fit a linear model to data as a simple example to get started and introduce some basic principles that we will use later for deep neural networks\n",
    "    1. Make a dataset\n",
    "    2. Create a model\n",
    "    3. Fit the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA\n",
    "   * Machine Learning models learn from examples which we will give to our model as a numpy arrays\n",
    "   * In supervised learning we'll have input variables $x$ and target variables $y$ (also know as ground truth)\n",
    "   * A data set will have pairs of input and output varibles in two arrays\n",
    "       * x.shape==[examples , input data ]\n",
    "       * y.shape==[examples , ground truth]   \n",
    "   * We're going to try and fit a line which will have one input variable $x$ and the output $y$\n",
    "       * $y=m*x+b$\n",
    "   * **Note** This is an example we normally won't know the true equation to calculate y\n",
    "       \n",
    "### Generate a dataset       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "x=np.random.uniform(-3,3,(2000,1)) #Generates 2,000 random numbers between -3,3 this is our input data\n",
    "\n",
    "slope=4\n",
    "intercept=3\n",
    "\n",
    "y=slope*x+intercept\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "plt.scatter(x[:,0],y[:,0])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give it a try\n",
    "\n",
    "use the cell below to create a dataset call my_x and my_y that\n",
    "* has 1,000 examples\n",
    "* has a slope of -1\n",
    "* has an intercept of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"your code here\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"run this to check your answer\"\"\"\n",
    "assert 'my_x' in locals(), \"my_x dosen't exist did you get the name correct\"\n",
    "print('my_x initialized')\n",
    "assert 'my_y' in locals(), \"my_y dosen't exist did you get the name correct\"\n",
    "print('my_y initialized')\n",
    "\n",
    "assert my_x.shape==(1000,1), \"my_x has the wrong shape\"\n",
    "print('my_x has the correct shape')\n",
    "assert my_y.shape==(1000,1), \"my_y has the wrong shape\"\n",
    "print('my_y has the correct shape')\n",
    "assert (my_x*-1+4==my_y).all(),  \"slope or intercept wrong\"\n",
    "print(\"the data have the correct slope and intercept\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Development, Testing     \n",
    "* We need to divide our examples into 3 groups            \n",
    "    * Training/Development/Testing\n",
    "    * Training: examples used to fit our models\n",
    "    * Development: examples for checking for over-fitting during training and development \n",
    "    * Testing: Examples for evaluating our final model but **not for making any modeling decisions**\n",
    "    \n",
    "**note -** you will often see only 2-splits: training and testing. This can be okay, but fails in some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=[]\n",
    "development=[]\n",
    "testing=[]\n",
    "for i in range(len(x)):\n",
    "    if np.random.uniform()>0.85:testing.append(i)\n",
    "    elif np.random.uniform()>0.7:development.append(i)\n",
    "    else: training.append(i)\n",
    "\n",
    "x_test=x[testing]        \n",
    "x_development=x[development]\n",
    "x_train=x[training]\n",
    "\n",
    "y_test=y[testing]        \n",
    "y_development=y[development]\n",
    "y_train=y[training]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Models can be anything from a simple linear model we'll use here to the most complicated deep-neural networks we're working toward. There are some commonalities\n",
    "  * Models have parameters $\\theta$ that control their output\n",
    "  * Models are fit to data, normally by minimizing a **loss function ($L$)**\n",
    "      * A Loss function ($L$) defines the goal of your model\n",
    "  * Models also have hyper-parameters $H$ that determine their structure and how they fit to data\n",
    "\n",
    "### Our simple model\n",
    "* $y_{pred,i}=\\theta_{1}*x_{i}+\\theta_{2} $\n",
    "* $L=\\frac{1}{N}\\sum_i (y_{pred,i}-y_{true,i})^2$\n",
    "* **Goal**: Find $\\theta_{1,2}$ that Minimizes $L$, or $min_\\theta(L)$\n",
    "\n",
    "Here $i$ represents each of our data points.\n",
    "\n",
    "**Pro-Tip** Since we made our data set we already know what $\\theta_{1}$ and $\\theta_{2}$ should be (the slope and intercept), but it's almost always a good idea to try your model on data you understand before running it on new more complicated data. If you can't get it working with the data you know, there is no chance of getting it working with the data you don't.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import linearmodel\n",
    "importlib.reload(linearmodel)\n",
    "\n",
    "\n",
    "model=linearmodel.LinearModel()\n",
    "\n",
    "print(\"Our initial unfit theta values:\",model.theta())\n",
    "\n",
    "y_pred=model.predict(x_train)\n",
    "\n",
    "plt.scatter(x_train[:,0],y_pred[:,0],label='prediction')\n",
    "plt.scatter(x_train[:,0],y_train[:,0],label='truth')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"With theta values of\", model.theta(), \"the loss is\", model.loss(x_train,y_train)) #Jake, check vocab\n",
    "model.settheta([2,3])\n",
    "print(\"With theta values of\", model.theta(), \"the loss is\", model.loss(x_train,y_train)) #Jake, check vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Loss Function\n",
    "Lets make a brute force plot of our loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=10\n",
    "plt_data=np.zeros((bins,bins))\n",
    "\n",
    "for x_pixel,theta1 in enumerate(np.linspace(0,10,bins)):\n",
    "    for y_pixel,theta2 in enumerate(np.linspace(0,10,bins)):\n",
    "        model.settheta([theta1,theta2])\n",
    "        plt_data[x_pixel][y_pixel]=model.loss(x_train,y_train)\n",
    "        \n",
    "        \n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(plt_data,cmap='Blues',interpolation='bilinear')\n",
    "plt.xlabel(r\"$\\theta_1$\")\n",
    "plt.ylabel(r\"$\\theta_2$\")\n",
    "plt.scatter([4],[3],marker='$X$',label='Minimum',color='r')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excersise\n",
    "\n",
    "Use the ```settheta()``` function, experiment with different values until you get a loss close to 10.\n",
    "\n",
    "or  $min_{\\theta}(L-10)^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.settheta([0,0])\n",
    "print(\"This should equal zero:\",(model.loss(x_train,y_train)-10)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing a loss function\n",
    "\n",
    "How did you attempt the exercise above? If you did like me you might have done something like\n",
    "\n",
    "* Try a number and see if the loss\n",
    "* Try another number and see if the loss is closer to 10\n",
    "    * If it is keep trying numbers in the same direction\n",
    "    * If it isn't change directions \n",
    "* Keep trying numbers in the same direction until the loss passes 10\n",
    "* Go back and try a number in-between the last two\n",
    "* etc.\n",
    "\n",
    "Machine learning algorithms work in a similar way\n",
    "\n",
    "* Decide what the right direction to change the weights is to minimize the loss\n",
    "    * It does this by calculating the **gradient**\n",
    "* Take a step in that direction\n",
    "    * How big a step? This is a **Hyper Parameter** called the **Learning Rate**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "The gradient $\\nabla_{\\theta} L$ is a vector with one entry per parameter $\\theta$ in your model, this vector points in the direction that increases the loss function. To minimize the loss function we want to take a step in the opposite direction.\n",
    "\n",
    "$\\theta_{new}=\\theta_{current}-\\alpha*\\nabla_{\\theta} L$\n",
    "\n",
    "$\\alpha$ controls how big a step along this vector.r\n",
    "\n",
    "\n",
    "The gradient is calculated by taking the partial derivative of the loss function with respect to the parameters.\n",
    "It can be done by hand in this case, but often this will be done for you by software package you are using.\n",
    "\n",
    "In this case for our model\n",
    "\n",
    "$L=\\frac{1}{N}\\sum_i (y_{pred,i}-y_{true,i})^2$\n",
    "\n",
    "$L=\\frac{1}{N}\\sum_i ((\\theta_{1}*x_{i}+\\theta_{2})-y_{true,i})^2$\n",
    "\n",
    "$dL/d\\theta_1= \\nabla_{\\theta} L_{1}= 2*\\frac{1}{N}\\sum_i ((\\theta_{1}*x_{i}+\\theta_{2})-y_{true,i})*x_{i}$          \n",
    "$dL/d\\theta_2= \\nabla_{\\theta} L_{2}=2*\\frac{1}{N}\\sum_i ((\\theta_{1}*x_{i}+\\theta_{2})-y_{true,i})$         \n",
    "\n",
    "## Learning Rate\n",
    "\n",
    "\n",
    "\n",
    "Run the cell below - it will run the code below and perform **steps=100** optimization steps, starting from a random theta where each step is:\n",
    "* Calculate: $\\nabla_{\\theta} L$\n",
    "* Update: $\\theta_{new}=\\theta_{current}-\\alpha*\\nabla_{\\theta} L$\n",
    "* Repeat\n",
    "\n",
    "and try different values of the learning rate lr ($\\alpha$ above) with big steps i.e. {1e-4,1e-3,1e-2,1e-1,...}\n",
    "You should see that too big a step can cause the optimization to fail, and too small a step can make the optimization take too long.\n",
    "\n",
    "* How big a learning rate can you use before the code below fails?\n",
    "* How small a learning rate does it take for the optimization to not reach the target in time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This cell starts with random values of theta and optimizes our linear model\"\n",
    "lr=1e-2\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)    #you can play with these values to make your plot bigger\n",
    "                                            #but if you go off the page, it may \"collapse\" to be small again\n",
    "\n",
    "plt.imshow(plt_data,cmap='Blues',interpolation='bilinear')\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    model.settheta([np.random.uniform(0,8),np.random.uniform(0,8)])\n",
    "    grads,path=model.optimize(x_train,y_train,lr,steps=100)\n",
    "    x_points,y_points=zip(*path)\n",
    "        \n",
    "    plt.scatter(x_points,y_points,marker='$O$')\n",
    "    plt.xlabel(r\"$\\theta_1$\")\n",
    "    plt.ylabel(r\"$\\theta_2$\")\n",
    "    \n",
    "    ax=plt.gca()\n",
    "\n",
    "    for (_x,_y),(_gx,_gy) in zip(path,grads):\n",
    "        ngx= -1*_gx*lr\n",
    "        ngy= -1*_gy*lr\n",
    "\n",
    "        ax.annotate(\"\", xy=(_x+ngx, _y+ngy), xytext=(_x, _y),\n",
    "             arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "       # plt.arrow(_x, _y, -1*ngx*lr, -1*ngy*lr)\n",
    "plt.scatter([4],[3],marker='$X$',label='Target',color='r')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic  Gradient Descent \n",
    "\n",
    "\n",
    "Above when we calculate $\\nabla_{\\theta} L$ we have an equation that looks like this\n",
    "\n",
    "$dL/d\\theta_1= \\nabla_{\\theta} L_{1}= 2*\\frac{1}{N}\\sum_i ((\\theta_{1}*x_{i}+\\theta_{2})-y_{true,i})*x_{i}$          \n",
    "\n",
    "However, the sum $\\sum_i$ over every data point $i$ can become very expensive especially for large datasets. To get around this we use a technique called Stochastic Gradient Descent. We use the same technique before, but with only a subset of the data. So instead of all examples we use only 20 (for example). The number we use is a new hyper-parameter **Batch Size**.\n",
    "\n",
    "**Batch Size** = Number of examples to use when calculating the gradient with Stochastic  Gradient Descent \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"This cell starts with random values of theta and optimizes our linear model\"\n",
    "lr=1e-2\n",
    "batch_size=1\n",
    "plt.imshow(plt_data,cmap='Blues',interpolation='bilinear')\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    model.settheta([np.random.uniform(0,8),np.random.uniform(0,8)])\n",
    "    grads,path=model.optimize(x_train,y_train,lr,steps=100,batch_size=batch_size)\n",
    "    x_points,y_points=zip(*path)\n",
    "        \n",
    "    plt.scatter(x_points,y_points,marker='$O$')\n",
    "    plt.xlabel(r\"$\\theta_1$\")\n",
    "    plt.ylabel(r\"$\\theta_2$\")\n",
    "    \n",
    "    ax=plt.gca()\n",
    "\n",
    "    for (_x,_y),(_gx,_gy) in zip(path,grads):\n",
    "        ngx= -1*_gx*lr\n",
    "        ngy= -1*_gy*lr\n",
    "\n",
    "        ax.annotate(\"\", xy=(_x+ngx, _y+ngy), xytext=(_x, _y),\n",
    "             arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "       # plt.arrow(_x, _y, -1*ngx*lr, -1*ngy*lr)\n",
    "plt.scatter([4],[3],marker='$X$',label='Target',color='r')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above with a batch size of 1, the path to the minimum wanders a bit more, but still converges. This is because each batch is a noisy estimate of the true loss function, and enough of them does a good job at approximating the complete loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other optimizers\n",
    "You maybe thinking \"there must be a better way of optimizing without using a fixed learning rate ($\\alpha$)!\". You're in luck! There are many strategies for optimization that use varying step sizes, and have their own different set of hyper parameters. Some you'll see:\n",
    "\n",
    "* Adam - A favorite of mine\n",
    "* RMSProp\n",
    "* Adagrad\n",
    "* Adadelta\n",
    "\n",
    "All use a similar strategy of stepping down a gradient, but how fast (or far) down the gradient is handled differently. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "* We've introduced a few concepts\n",
    "    * A model, which is a way of approximating a function from data\n",
    "        * It has parameters $\\theta$\n",
    "        * It has hyper-parameters like\n",
    "            * Learning Rates\n",
    "            * Batch Sizes\n",
    "    * Introduced how a model is fit to data\n",
    "        * Gradient Descent\n",
    "        *  Stochastic Gradient Descent\n",
    "* Next lecture we will create our own model with a neural network, and train it to fit simple data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
