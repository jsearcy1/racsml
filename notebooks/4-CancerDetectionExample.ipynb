{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camelyonpatch import CamelyOnPatch\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real world Bio-Medical Datasets\n",
    "\n",
    "This notebook is a start on how you can use machine learning on bio-medical datasets.\n",
    "\n",
    "## Tumor Identification Segmentation\n",
    "\n",
    "We're going to start with the PatchCamelyon (PCam)\n",
    "https://github.com/basveeling/pcam\n",
    "\n",
    "<img src=https://raw.githubusercontent.com/basveeling/pcam/master/pcam.jpg>\n",
    "\n",
    "\n",
    "## The RAW Data\n",
    "\n",
    "This dataset was created from whole slide images for example this one:\n",
    "* Whole slide image scans are gigantic!\n",
    "    * Really Big     (i.e. in this example 97,792 pixels by 221,184 pixels\n",
    "        * ~21,000 MegaPixels\n",
    "* Below is a low resolution view\n",
    "* These image don't generally fit into memory, but python packages exist to manipulate them\n",
    "\n",
    "For this file we're relying on a package called openslide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openslide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=../assets/full_slide.png>\n",
    "Labels in this dataset are the boundaries of each tumor.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image in full Resolution is too large to fit in memory, but it doesn't mean we can't still use what we've learned to start analyzing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Dataset\n",
    "\n",
    "Instead of looking at the whole image we can look at one small piece say 96x96 pixels worth. \n",
    "* Ask are any of the pixels in the middle 32x32 cancerous\n",
    "    * If yes label it with a 1\n",
    "    * If no label it with a zero\n",
    "    * Why only the middle 32x32 pixels\n",
    "        * Allows network to have some context to understanding the center pixels\n",
    "        * Otherwise one pixel in the top corner being cancerous would label the whole image cancerous\n",
    "* This turns one picture into 1000s of trainning examples\n",
    "\n",
    "* **Important Detail** about the data split\n",
    "    * Having a lot of training examples is great\n",
    "        * But what we really want to know is can we use this algorithm to identify tumors it hasn't seen before\n",
    "        * The dataset used above splits the data by randomly assigning each tumor to a dataset rather than randomly assigning each image to a dataset\n",
    "            * Make sure the model gernalizes to new tumors and not just those seen in the training set\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpd=CamelyOnPatch('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/')\n",
    "\n",
    "plt.imshow(cpd.X_train[0])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is just like the examples from the last lecture, but with slightly larger images\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Try writing your own model, and training it \n",
    "* Start with an input layer\n",
    "* Add Convolutional layers\n",
    "    * Remember to add an activation\n",
    "* Downsample with either pooling or striding\n",
    "* Make a keras model, call it **model**\n",
    "    * compile the model including accuracy as a metric \n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Create your model here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Add model training code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer these Questions\n",
    "\n",
    "* How many parameters were in your model?\n",
    "* How long did an epoch take?\n",
    "* What accuracy were you able to reach?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Simple CNN\n",
    "\n",
    "* If you want to use your own model written above, go for it! (just skip the cell below)\n",
    "* Otherwise use the network below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=input(\"Are you sure you want to use my model? Y/N\")\n",
    "if ans.lower() == 'y':\n",
    "    cnn_input=tf.keras.layers.Input( shape=cpd.X_train.shape[1:] ) # Shape here does not including the batch size \n",
    "    cnn_layer1=tf.keras.layers.Convolution2D(64, (2,2),strides=2,padding='same')(cnn_input) \n",
    "    cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer1) \n",
    "    cnn_activation=tf.keras.layers.Dropout(0.3)(cnn_activation) \n",
    "\n",
    "    cnn_layer2=tf.keras.layers.Convolution2D(126, (2,2),strides=2,padding='same')(cnn_activation) \n",
    "    cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer2) \n",
    "    cnn_activation=tf.keras.layers.Dropout(0.3)(cnn_activation) \n",
    "\n",
    "    cnn_layer3=tf.keras.layers.Convolution2D(256, (2,2),strides=2,padding='same')(cnn_activation) \n",
    "    cnn_activation=tf.keras.layers.LeakyReLU()(cnn_layer3) \n",
    "    cnn_activation=tf.keras.layers.Dropout(0.3)(cnn_activation) \n",
    "\n",
    "\n",
    "    flat=tf.keras.layers.Flatten()(cnn_activation) \n",
    "\n",
    "    drop=tf.keras.layers.Dropout(0.3)(flat) \n",
    "\n",
    "    dense_layer=tf.keras.layers.Dense(1)(drop) \n",
    "    output=tf.keras.layers.Activation('sigmoid')(dense_layer)\n",
    "\n",
    "    model=tf.keras.models.Model([cnn_input],[output])\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.load_weights('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/cnn_1.h5')\n",
    "\n",
    "    if False:\n",
    "        history=model.fit(cpd.X_train, cpd.Y_train, \n",
    "              batch_size=100, epochs=10, verbose=1,\n",
    "             validation_data=(cpd.X_develop,cpd.Y_develop),\n",
    "                      shuffle='batch'\n",
    "             )\n",
    "        model.save_weights('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/cnn_1.h5')\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a plot of our predictions\n",
    "\n",
    "Let's check to see how well our model is working\n",
    " * We'll look at the same histogram we did in previous lectures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.squeeze(model.predict(cpd.X_develop))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=plt.figure(figsize=(10,10))\n",
    "tumor=[pred for pred,truth in zip(y_pred,cpd.Y_develop) if truth  ]\n",
    "fine=[pred for pred,truth in zip(y_pred,cpd.Y_develop) if not truth  ]\n",
    "\n",
    "plt.hist(tumor,range=(0,1),bins=200,density=True,histtype='step',label='Tumor')  \n",
    "plt.hist(fine,range=(0,1),bins=200,density=True,histtype='step',label='Normal')\n",
    "plt.xlabel('Neural Network Response')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopefully there is a clear separation between tumor and normal\n",
    "\n",
    "## Is it good enough?\n",
    "\n",
    "In order to answer this question it's important to decide how we want to use it\n",
    "\n",
    "**Goal**: Assist in diagnosis by highlighting potential tumors.\n",
    "\n",
    "Let's see if it's good enough by giving it a try\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- BREAK ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole Image Scan\n",
    "* Let's try using our model on a whole tissue slide\n",
    "\n",
    "## Slide Data\n",
    "\n",
    "* The data we're using is stored in .tif files that are very large\n",
    "    * Too large to fit into memory\n",
    "    * Too large to easily visualize \n",
    "* We'll use https://openslide.org/ to read these files\n",
    "\n",
    "* Files have levels of magnification from 0 (full-res) to 9 (low res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a full slide image\n",
    "tiff_file='/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/tumor_001.tif'\n",
    "slide_image=openslide.OpenSlide(tiff_file)\n",
    "\n",
    "# Slide images are divided into levels with level 0 being the highest resolution\n",
    "for i,res in enumerate(slide_image.level_dimensions):\n",
    "    print('The Resolution at level',i, 'is', res)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the index coordinate system\n",
    "\n",
    "<img src=../assets/index_coords.png>\n",
    "\n",
    "\n",
    "Note that for open slide we need to use the index coordinates for **level 0**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#We can read each region of the image at one time at different levels\n",
    "\n",
    "#Top left_pixel (x,y) in level 0, level,width,height\n",
    "print(\"Full Image at level 8\")\n",
    "image=np.asarray(slide_image.read_region( (0,0),8,(382, 864)  ))\n",
    "f=plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i,v in enumerate(slide_image.level_dimensions):\n",
    "    print(\"Level\",i,\"Resolution\",v)\n",
    "    f=plt.figure(figsize=(10,10))\n",
    "    image=np.asarray(slide_image.read_region( (50000,120000),i,(500,500)  ))\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use open-slide to grab 96x96x3 pixel chunks and feed them to our model\n",
    "\n",
    "Our training Data was split into 96x96 pixel images at level-2\n",
    " * Let's check to make sure that's not crazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cpd.X_train[0])\n",
    "plt.show()\n",
    "image=np.asarray(slide_image.read_region( (50000,120000),2,(96,96)  ))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do they look similar in resolution?\n",
    "\n",
    "### How many predictions do we need to make?\n",
    "\n",
    "Remember, each training example is 96x96 but only the middle 32x32 region counts for the label.\n",
    "\n",
    "We'll step in 32 pixel steps to generate a new image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is the resolution at level 2\n",
    "initial_dimension=slide_image.level_dimensions[2]\n",
    "\n",
    "# This is the resolution after we've turned each 32x32 pixel box into a predictions\n",
    "final_dimension= np.array(initial_dimension)/32.\n",
    "\n",
    "print(\"prescan dimension\",initial_dimension,\"final scan\",final_dimension)\n",
    "n_predictions=np.product(final_dimension)\n",
    "print(n_predictions/1e6,\" Million Predictions Required\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a lot of predictions\n",
    "\n",
    "It's possible to do the whole slide, but it takes awhile (30 minutes or so...) let's try a smaller section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#These coordinates are at level 0\n",
    "width=6400\n",
    "height=6400\n",
    "\n",
    "x_start=60000\n",
    "y_start=120000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Predictions Required\",width//32*height//32)\n",
    "\n",
    "\n",
    "#Covert these for later use to level2\n",
    "level0_dimension=slide_image.level_dimensions[0]\n",
    "level2_dimension=slide_image.level_dimensions[2]\n",
    "\n",
    "sfactor_x=level0_dimension[0]/level2_dimension[0]\n",
    "sfactor_y=level0_dimension[1]/level2_dimension[1]\n",
    "\n",
    "x_stop=x_start+width*4\n",
    "y_stop=y_start+height*4\n",
    "\n",
    "\n",
    "print('Scaling factor to level 2',sfactor_x,sfactor_y)\n",
    "\n",
    "# It's just a factor of 4\n",
    "f=plt.figure(figsize=(50,50))\n",
    "plt.imshow(slide_image.read_region( (x_start,y_start),2,(height,width)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not bad - let's scan this image\n",
    "How can we feed this to our model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Generator\n",
    "\n",
    "python has a neat concept called a generator that we can use in our ML models\n",
    "* A function that generates data each time it is called\n",
    "* Often a while loop the loops forever \n",
    "* uses the **yield** keyword\n",
    "* Each new element can be grabbed with the next keyword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_random_numbers():\n",
    "    while True:\n",
    "        yield( np.random.uniform(size=5))\n",
    "my_generator=five_random_numbers()\n",
    "\n",
    "for i in range(10):\n",
    "    print(next(my_generator))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give it a try\n",
    "We'll use a generator to scan our input slide, try writing one that returns batch_sizex96x96x3 random images\n",
    "\n",
    "`hint np.random.normal(size(batch_size,96x96x3))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def my_generator(\"Your code from here\"\n",
    "\n",
    "    \n",
    "                 \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Run this to check\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Generator to load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scan_image(image_file,batch_size,x_range,y_range):\n",
    "    slide_image=openslide.OpenSlide(image_file)\n",
    "    res_x,res_y=slide_image.level_dimensions[2]\n",
    "    \n",
    "    coord_x,coord_y=slide_image.level_dimensions[0] \n",
    "    #This is factor we need to scale the pixels at resolution 2 to the coordinates in resolution 0\n",
    "\n",
    "    sfactor_x=coord_x/res_x  \n",
    "    sfactor_y=coord_y/res_y \n",
    "    \n",
    "    batch=[]\n",
    "    index=0\n",
    "    for x in range(x_range[0]//4,x_range[1]//4,32):\n",
    "        for y in range(y_range[0]//4,y_range[1]//4,32):\n",
    "            image=np.asarray(slide_image.read_region( (int(x*sfactor_x),int(y*sfactor_y)),2,(96,96)  ))/255.\n",
    "            batch.append(np.expand_dims(image[:,:,0:3],0))                                \n",
    "            if len(batch)==batch_size:\n",
    "                yield(np.concatenate(batch,0))\n",
    "                batch=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "x_range=[x_start,x_stop]\n",
    "y_range=[y_start,y_stop]\n",
    "\n",
    "n_predictions=width//32*height//32\n",
    "steps=n_predictions//batch_size\n",
    "\n",
    "generator=scan_image('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/tumor_001.tif',batch_size,x_range,y_range)\n",
    "steps=n_predictions//batch_size\n",
    "\n",
    "output_scan=model.predict_generator(generator,steps)\n",
    "output_map=np.zeros((height//32,width//32))\n",
    "print(output_scan)\n",
    "for index,v in enumerate(output_scan):\n",
    "    \n",
    "    y=index%(width//32)\n",
    "    x=index//(width//32)\n",
    "    output_map[y,x]=v\n",
    "    \n",
    "plt.imshow(output_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are probabilites\n",
    "Let's check which are greater than 50%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f=plt.figure(figsize=(2,2))\n",
    "plt.imshow(output_map > 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Manipulating Data\n",
    "\n",
    "1. Use Open Slide to find the coordinates of a different region of cells\n",
    "2. Use the coordinates, The generator code above, and our model to predict a new region\n",
    "3. What diagnosis would you make (or would you?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hmmm\n",
    "The full slide looks like this (as you can guess from the file name it has a tumor)\n",
    "<img src='../assets/full_slide_scan_1.png'>\n",
    "\n",
    "But wait a slide without a tumor looks like this\n",
    "\n",
    "<img src='../assets/full_slide_scan_normal.png'>\n",
    "\n",
    "\n",
    "The results above don't look great, lots of likely Tumors even in places without cells!\n",
    "\n",
    "* Our training data didn't include things like slide edges and slide area's without tissues\n",
    "    * The results on these can be fairly random\n",
    "* A bigger issue is a problem with statistics\n",
    "\n",
    "* Remember we are approximating    \n",
    "    $P(y|x)$\n",
    "\n",
    "* Which using Bayesâ€™ theorem\n",
    "\n",
    "    $P(y|x)=\\frac{P(x|y)P(y)}{P(x)}$\n",
    "\n",
    "* What is P(y) in our dataset?\n",
    "    * This the probability an image contained at least one tumor pixel\n",
    "    * Our dataset was artifically built so $P(y=tumor)=1/2$ and $P(y=healthy)=1/2$\n",
    "        * This makes training easier\n",
    "        * This is called class Balancing\n",
    "    * However, looking at cells in the wild it is vastly more likely that they are non-cancerous\n",
    "* How do we use these predictions?\n",
    "    * We know $P(y)$ is quite a bit smaller in the real world\n",
    "    * We $P(y|x)$ is porportional to $P(y)$\n",
    "        * The real world $P(y|x)$ is smaller than the one measured in our experiment\n",
    "    * An easy way to reduce false positives is to increase the threshold we used from 50% to something larger\n",
    "        * How high is a judgment cell\n",
    "        * Too high could miss tumors\n",
    "        * Too low and there are fake tumors everywhere\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for threshhold in [0.5,0.7,0.8,0.9,0.95,0.99]:    \n",
    "    print('Threshhold',threshhold)\n",
    "    f=plt.figure(figsize=(5,12))\n",
    "    plt.imshow(output_map > threshhold)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How confident would you be in this diagnosis?\n",
    "\n",
    "Probably not very, so let's try to do better\n",
    "\n",
    "# Improving our models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Add Data\n",
    "\n",
    "Problem: Too many false positives even in areas without Cells!\n",
    "\n",
    "Any easy way to get rid of false positives is to add some data\n",
    "* Our original dataset removed any image that didn't contain cells\n",
    "* We can add a few back in\n",
    "\n",
    "##  Data Augmentation \n",
    "With limited data over-fitting can be an issue. One way to get around this is to make new data.\n",
    "\n",
    "* We can manipulate our existing images to generate new ones\n",
    "    * We can rotate them\n",
    "    * We can mirror them\n",
    "    * We can shift them\n",
    "    * etc\n",
    "* We know that shifting the cells won't change their tumor status, but the model doesn't, so this does help as long as the manipulations don't alter the data too much (if it does, you won't learn anything and the develop dataset won't improve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Image Data Generator\n",
    "\n",
    "This is a built in generator like the one we used to scan our full slide, but it has the ability to change images on the fly, so no image you use in training is exactly the same.\n",
    "\n",
    "```\n",
    "keras.preprocessing.image.ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False, \n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False, \n",
    "    zca_epsilon=1e-06, \n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.0, \n",
    "    height_shift_range=0.0, \n",
    "    brightness_range=None, \n",
    "    shear_range=0.0,\n",
    "    zoom_range=0.0, \n",
    "    channel_shift_range=0.0, \n",
    "    fill_mode='nearest', \n",
    "    cval=0.0, \n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=None, \n",
    "    preprocessing_function=None, \n",
    "    data_format=None, \n",
    "    validation_split=0.0, \n",
    "    dtype=None)\n",
    "```\n",
    "\n",
    "## Input\n",
    "\n",
    "There are several ways of using this, but we're going to use raw (96x96x3) images stored in folders.\n",
    "Classes are identifided in sub-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rescale=1./255.,\n",
    "            width_shift_range=0,  # randomly shift images horizontally\n",
    "            height_shift_range=0,  # randomly shift images vertically \n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=True,\n",
    "            shear_range=1,\n",
    "            zoom_range=.05,\n",
    "            rotation_range=15                           \n",
    "            )  # randomly flip images\n",
    "train_generator=data_gen.flow_from_directory('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/train',\n",
    "                                            target_size=(96,96), \n",
    "                                            color_mode='rgb', \n",
    "                                            classes=['normal','tumor'],\n",
    "                                            class_mode='binary',\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=True)\n",
    "\n",
    "valid_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rescale=1./255.,\n",
    "            width_shift_range=0,  # randomly shift images horizontally\n",
    "            height_shift_range=0,  # randomly shift images vertically \n",
    "            horizontal_flip=False,  # randomly flip images\n",
    "            vertical_flip=False,\n",
    "            shear_range=0,\n",
    "            zoom_range=.00,\n",
    "            rotation_range=0                          \n",
    "            )  # randomly flip images\n",
    "\n",
    "develop_generator=valid_gen.flow_from_directory('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/develop',\n",
    "                                            target_size=(96,96), \n",
    "                                            color_mode='rgb', \n",
    "                                            classes=['normal','tumor'],\n",
    "                                            class_mode='binary',\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=False)\n",
    "\n",
    "train_generator=valid_gen.flow_from_directory('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/develop',\n",
    "                                            target_size=(96,96), \n",
    "                                            color_mode='rgb', \n",
    "                                            classes=['normal','tumor'],\n",
    "                                            class_mode='binary',\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=False)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_images,aug_label=next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(aug_images[25])\n",
    "print(aug_label)\n",
    "print(aug_images)\n",
    "np.max(aug_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/cnn_2.h5')\n",
    "if False:\n",
    "    \n",
    "    model.fit_generator(train_generator,steps_per_epoch=1000,epochs=10,validation_data=(cpd.X_develop,cpd.Y_develop),\n",
    ")\n",
    "    model.save_weights('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/cnn_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "x_range=[x_start,x_stop]\n",
    "y_range=[y_start,y_stop]\n",
    "\n",
    "n_predictions=width//32*height//32\n",
    "steps=n_predictions//batch_size\n",
    "\n",
    "generator=scan_image('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/tumor_001.tif',batch_size,x_range,y_range)\n",
    "steps=n_predictions//batch_size\n",
    "\n",
    "output_scan=model.predict_generator(generator,steps)\n",
    "output_map=np.zeros((height//32,width//32))\n",
    "print(output_scan)\n",
    "for index,v in enumerate(output_scan):\n",
    "    \n",
    "    y=index%(width//32)\n",
    "    x=index//(width//32)\n",
    "    output_map[y,x]=v\n",
    "    \n",
    "plt.imshow(output_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(output_map.shape)\n",
    "print(final_dimension)\n",
    "f=plt.figure(figsize=(10,30))\n",
    "plt.imshow(output_map>0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit better\n",
    "\n",
    "\n",
    "We can make the model better by adding layers, better data augmentation,\n",
    "trying out tricks we find on stack overflow. There's a better way.\n",
    "\n",
    "\n",
    "# Don't be a Hero\n",
    "Sometimes a simple model is all you need, but when you want to do the best you can, you can use an existing model made by an ML research team, and trained on very large image datasets.\n",
    "\n",
    "\n",
    "https://keras.io/applications/\n",
    "\n",
    "|Model \t |    Size |\tTop-1 Accuracy|\tTop-5 Accuracy |\tParameters |\tDepth|\n",
    "|----|----|----|----|----|----|\n",
    "|Xception|88 MB |\t0.790         | \t0.945|   22,910,480|     \t126|\n",
    "|VGG16 \t     |       528 MB |\t0.713| \t0.901| \t138,357,544| \t23      \n",
    "|VGG19 \t      |      549 MB |\t0.713| \t0.900| \t143,667,240| \t26\n",
    "|ResNet50 \t   |     98 MB \t|0.749| \t0.921| \t25,636,712| \t-\n",
    "|ResNet101 \t    |    171 MB |\t0.764| \t0.928| \t44,707,176| \t-\n",
    "|ResNet152 \t     |   232 MB |\t0.766| \t0.931|\t60,419,944| \t-\n",
    "|ResNet50V2 \t      |  98 MB \t|0.760| 0.930| \t25,613,800| \t-\n",
    "|ResNet101V2        | 171 MB |\t0.772| \t0.938| \t44,675,560| \t-\n",
    "|ResNet152V2         |232 MB |\t0.780| \t0.942| \t60,380,648| \t-\n",
    "|ResNeXt50 \t|        96 MB \t|0.777| \t0.938| \t25,097,128| \t-\n",
    "|ResNeXt101 \t |       170 MB |0.787| 0.943| \t44,315,560| \t-\n",
    "|InceptionV3   |      92 MB \t|0.779| 0.937| \t23,851,784| \t159\n",
    "|InceptionResNetV2| \t215 MB \t|0.803| 0.953| \t55,873,736| \t572\n",
    "|MobileNet \t   |     6 MB \t|0.704| \t0.895| \t4,253,864| \t88\n",
    "|MobileNetV2 |\t    14 MB \t|0.713| \t0.901| \t3,538,984| \t88\n",
    "|DenseNet121 |\t    33 MB \t|0.750| \t0.923| \t8,062,504| \t121\n",
    "|DenseNet169 |\t    57 MB \t|0.762| \t0.932| \t14,307,880| \t169\n",
    "|DenseNet201 |\t    80 MB \t|0.773| \t0.936|\t20,242,984| \t201\n",
    "|NASNetMobile |\t    23 MB \t|0.744| \t0.919| \t5,326,716| \t-\n",
    "|NASNetLarge |\t343 MB \t    |0.825| \t0.960| \t88,949,818| \t-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example VGG16\n",
    "\n",
    "<img src=https://neurohive.io/wp-content/uploads/2018/11/vgg16.png>\n",
    "\n",
    "This model was trained on ImageNet (http://www.image-net.org/)\n",
    "* Classification of a thousand of different objects, and trained with Millions of images\n",
    "* We want to take all those pre-trained weights and use them in our cancer example\n",
    "\n",
    "\n",
    "Remember from the models we wrote\n",
    "   * We have a stack of Convolutional Layers\n",
    "   * Flatten Layer\n",
    "   * Dense Layer(s) to make predictions\n",
    "\n",
    "Since the classification is only at the end, we can just remove it\n",
    "   * In the figure above these are all the layers in green\n",
    "   * We can replace it with new layers\n",
    "\n",
    "You'll hear this called **transfer learning**, because we're using the features learned from ImageNet to do classification on our data.\n",
    "\n",
    "**Important** we've been scaling our images from 0-1 by dividing by 255. This isn't the way it's always done, and if you don't use the right image pre-processing functions for the model you're using you might get meaningful looking results but ones that are very sub-optimal.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre_trained=tf.keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',input_tensor=None, input_shape=(96,96,3), pooling=None)\n",
    "\n",
    "#Load a pretrained model\n",
    "\n",
    "#set include_top to false this removes the dense layers at the end\n",
    "pre_trained=tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet',input_tensor=None, input_shape=(96,96,3), pooling=None)\n",
    "preprocessing=tf.keras.applications.vgg16.preprocess_input\n",
    "\n",
    "\n",
    "# fix all these layers so we don't train them right away\n",
    "for l in pre_trained.layers:\n",
    "    l.trainable=False\n",
    "\n",
    "\n",
    "# Add our own new Dense Layers    \n",
    "flat=tf.keras.layers.Flatten()(pre_trained.output)\n",
    "top=tf.keras.layers.Dense(256)(flat)\n",
    "top=tf.keras.layers.LeakyReLU()(top)\n",
    "top=tf.keras.layers.Dropout(0.3)(top)\n",
    "\n",
    "classification=tf.keras.layers.Dense(1,activation='sigmoid')(top)\n",
    "\n",
    "# Model/compile like before\n",
    "\n",
    "\n",
    "model=tf.keras.models.Model([pre_trained.input],classification)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(lr=1e-4),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make generators like before but now using the preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            preprocessing_function=preprocessing,\n",
    "            width_shift_range=0,  # randomly shift images horizontally\n",
    "            height_shift_range=0,  # randomly shift images vertically \n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=True,\n",
    "            shear_range=1,\n",
    "            zoom_range=.05,\n",
    "            rotation_range=15                           \n",
    "            )  # randomly flip images\n",
    "\n",
    "train_generator=data_gen.flow_from_directory('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/train',\n",
    "                                            target_size=(96,96), \n",
    "                                            color_mode='rgb', \n",
    "                                            classes=['normal','tumor'],\n",
    "                                            class_mode='binary',\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=True)\n",
    "\n",
    "develop_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            preprocessing_function=preprocessing,\n",
    "            width_shift_range=0,  # don't do anything but preprocess\n",
    "            height_shift_range=0,  \n",
    "            horizontal_flip=False,  \n",
    "            vertical_flip=False,\n",
    "            shear_range=0,\n",
    "            zoom_range=.00,\n",
    "            rotation_range=0                          \n",
    "            )  # randomly flip images\n",
    "\n",
    "develop_generator=develop_gen.flow_from_directory('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/develop',\n",
    "                                            target_size=(96,96), \n",
    "                                            color_mode='rgb', \n",
    "                                            classes=['normal','tumor'],\n",
    "                                            class_mode='binary',\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=False)\n",
    "\n",
    "test_generator=develop_gen.flow_from_directory('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/images/test',\n",
    "                                            target_size=(96,96), \n",
    "                                            color_mode='rgb', \n",
    "                                            classes=['normal','tumor'],\n",
    "                                            class_mode='binary',\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=False)\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try_fit = True\n",
    "\n",
    "if try_fit:\n",
    "    history=model.fit_generator(train_generator,steps_per_epoch=1000,epochs=5,validation_data=develop_generator,validation_steps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if try_fit:\n",
    "    for l in pre_trained.layers:\n",
    "        l.trainable=True\n",
    "    \n",
    "    # You must! recompile after freezing layers\n",
    "    model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(lr=1e-4),metrics=['accuracy'])\n",
    "\n",
    "    model.fit_generator(train_generator,steps_per_epoch=1000,epochs=10,validation_data=develop_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not try_fit:\n",
    "    model.load_weights('/projects/bgmp/shared/2019_ML_workshop/VGG16.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=np.squeeze(develop_generator.classes)\n",
    "y_pred=np.squeeze(model.predict_generator(develop_generator))\n",
    "\n",
    "fine=[pred for pred,truth in zip(y_pred,y_true) if not truth  ]\n",
    "tumor=[pred for pred,truth in zip(y_pred,y_true) if  truth  ]\n",
    "\n",
    "plt.hist(tumor,range=(0,1),bins=200,density=True,histtype='step',label='tumor')\n",
    "plt.hist(fine,range=(0,1),bins=200,density=True,histtype='step',label='normal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scan_image(image_file,batch_size,x_range,y_range,preprocessing_function):\n",
    "    slide_image=openslide.OpenSlide(image_file)\n",
    "    res_x,res_y=slide_image.level_dimensions[2]\n",
    "    \n",
    "    coord_x,coord_y=slide_image.level_dimensions[0] \n",
    "    #This is factor we need to scale the pixels at resolution 2 to the coordinates in resolution 0\n",
    "\n",
    "    sfactor_x=coord_x/res_x  \n",
    "    sfactor_y=coord_y/res_y \n",
    "    \n",
    "    batch=[]\n",
    "    index=0\n",
    "    for x in range(x_range[0]//4,x_range[1]//4,32):\n",
    "        for y in range(y_range[0]//4,y_range[1]//4,32):\n",
    "            image=np.asarray(slide_image.read_region( (int(x*sfactor_x),int(y*sfactor_y)),2,(96,96)  ))\n",
    "            batch.append( preprocessing_function(np.expand_dims(image[:,:,0:3],0))  )                                \n",
    "\n",
    "            if len(batch)==batch_size:\n",
    "                yield(np.concatenate(batch,0))\n",
    "                batch=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "x_range=[x_start,x_stop]\n",
    "y_range=[y_start,y_stop]\n",
    "\n",
    "n_predictions=width//32*height//32\n",
    "steps=n_predictions//batch_size\n",
    "\n",
    "generator=scan_image('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/tumor_001.tif',batch_size,x_range,y_range,preprocessing)\n",
    "steps=n_predictions//batch_size\n",
    "\n",
    "output_scan=model.predict_generator(generator,steps)\n",
    "output_map=np.zeros((height//32,width//32))\n",
    "print(output_scan)\n",
    "for index,v in enumerate(output_scan):\n",
    "    \n",
    "    y=index%(width//32)\n",
    "    x=index//(width//32)\n",
    "    output_map[y,x]=v\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "plt.imshow(output_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Slides \n",
    "\n",
    "This slide with a tumor\n",
    "\n",
    "<img src=\"../assets/full_slide_scan_vgg16.png\">\n",
    "\n",
    "A slide with no tumor\n",
    "\n",
    "<img src=\"../assets/full_slide_scan_vgg16_normal.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking a lot better\n",
    "\n",
    "Still some false positives along the slide/cell edges\n",
    "* We could try to harvest more of that data\n",
    "* Or we could institute a policy\n",
    "    * Don't use the model for cell on the edge of a slide\n",
    "* A human will make the diagnosis and the better the model the faster and more reliable it will be\n",
    "* How did our model do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where is the tumor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('/projects/bgmp/shared/2019_ML_workshop/datasets/pcamv1/tumor_001.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "tumor_coords=[(float(el.attrib['X']),float(el.attrib['Y'])) for el in root.findall('Annotations/Annotation/Coordinates/Coordinate')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tumor_x=[(x-x_start)/4./32.-2 for x,y in tumor_coords]\n",
    "tumor_y=[(y-y_start)/4./32.-2 for x,y in tumor_coords]\n",
    "f=plt.figure(figsize=(30,30))\n",
    "plt.imshow(output_map)\n",
    "plt.scatter(tumor_x,tumor_y)\n",
    "\n",
    "f=plt.figure(figsize=(30,30))\n",
    "plt.imshow(slide_image.read_region( (x_start,y_start),2,(height,width)))\n",
    "tumor_x=[(x-x_start)/4. for x,y in tumor_coords]\n",
    "tumor_y=[(y-y_start)/4. for x,y in tumor_coords]\n",
    "plt.scatter(tumor_x,tumor_y)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We did highlight both tumors, but we also highlighed a different area of cells\n",
    "\n",
    "* Group Question, how would you try and fix this issue?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Can you do even better?\n",
    "\n",
    "Give it a try with a different pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary for using existing models\n",
    "\n",
    "* Be careful not to 'blow up' existing well trained weights\n",
    "* Train the dense layers first classifier\n",
    "* Fine-Tune  other layers (use small learning rates)\n",
    "* don't overfit! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
